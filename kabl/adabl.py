#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MODULE RELATIVE TO SUPERVISED LEARNING FUNCTIONS FOR THE KABL PROGRAM.

 +-----------------------------------------+
 |  Date of creation: 25 Nov. 2019         |
 +-----------------------------------------+
 |  Meteo-France                           |
 |  DSO/DOA/IED and CNRM/GMEI/LISA         |
 +-----------------------------------------+
 
"""

# Usual Python packages
import numpy as np
import pandas as pd
import datetime as dt
import sys
import os.path
import time
import pickle
import netCDF4 as nc

# Machine learning packages
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# Local packages
from kabl import paths
from kabl import core
from kabl import utils
from kabl import graphics


def add_blhs_to_netcdf(ncFile, blhs, blhs_names):
    """Add new fields to a netCDF file.
    
    
    Parameters
    ----------
    ncFile : str
        Path to the netcdf file containing the data
    
    blhs : list of array-like
        Fields to add to the netcdf
    
    blhs_names : list of str
        Names of the fields to add
    
    
    Returns
    -------
    None
    """

    ncf = nc.Dataset(ncFile, "r+")

    # Fill in BLHs vectors
    for ib in range(len(blhs)):
        BLH = ncf.createVariable(blhs_names[ib], np.float64, ("time",))
        BLH[:] = blhs[ib][:]
        BLH.units = "Meters above ground level (m agl)"

    ncf.close()

    return "Fields " + str(blhs_names) + " successfully added to " + ncFile


def prepare_supervised_dataset(
    dataFiles: list,
    refFiles: list,
    saveInCSV: bool = False,
    outputFile: str = None,
    plot_on: bool = False,
):
    """Create a dataframe with appropriate fields from original data format.
    
    Lidar data is expected to be provided in raw2l1 files and handmade BLH 
    estimation is expected in .csv file with 2 columns: time, BLH values.
    Paths are given in a list in order to easily had multiple days.
    
    
    Parameters
    ----------
    dataFile : list of str
        Paths to the data input file, as generated by raw2l1
    
    refFile : list of str
        Paths to the reference file (handmade BLH estimation) in CSV format
    
    saveInCSV : bool, default=False
        If True, the dataset is saved in a .csv file at the specified location
        
    outputFile : str, default=None
        Path to the file where the dataset is stored, if saveInCSV=True
    
    plot_on : bool, default=False
        If True, display the handmade BLH over the data.
    
    
    Returns
    -------
    df : `pandas.DataFrame`
        Ready-to-use dataframe for ADABL training. Contains 5 columns of input
        data and one column of output binary data
    """

    RCS0 = []
    RCS1 = []
    RCS2 = []
    SEC0 = []
    ALTI = []
    y = []
    for i in range(len(dataFiles)):
        dataFile = dataFiles[i]
        refFile = refFiles[i]
        print("Reading file ", dataFile, "with reference", refFile)
        t_values, z_values, dat = utils.extract_data(
            dataFile, max_height=4620, to_extract=["rcs_0", "rcs_1", "rcs_2", "pbl"]
        )
        rcs_0 = dat["rcs_0"]
        rcs_1 = dat["rcs_1"]
        rcs_2 = dat["rcs_2"]
        blh_mnf = dat["pbl"]

        blh_ref = pd.read_csv(refFile, delimiter=",", header=0)
        blh_ref = blh_ref["blh_ref"].values

        if plot_on:
            graphics.blhs_over_data(t_values, z_values, rcs_0, blh_ref)

        # Input data
        # ----------
        sec_intheday = np.mod(t_values, 24 * 3600)
        Nt, Nz = rcs_1.shape

        rcs0loc = rcs_0.ravel()
        rcs0loc[rcs0loc <= 0] = 1e-5
        RCS0.append(np.log10(rcs0loc))

        rcs1loc = rcs_1.ravel()
        rcs1loc[rcs1loc <= 0] = 1e-5
        RCS1.append(np.log10(rcs1loc))

        rcs2loc = rcs_2.ravel()
        rcs2loc[rcs2loc <= 0] = 2e-5
        RCS2.append(np.log10(rcs2loc))

        SEC0.append(np.repeat(sec_intheday, Nz))
        ALTI.append(np.tile(z_values, Nt))

        # Output data
        # -----------
        yday = []
        for t in range(Nt):
            yloc = np.zeros(Nz)
            yloc[z_values > blh_ref[t]] = 1
            yday.append(yloc)

        y.append(np.array(yday, dtype=int).ravel())

    # Create dataframe
    # ------------------
    df = pd.DataFrame(
        {
            "sec0": np.concatenate(SEC0),
            "alti": np.concatenate(ALTI),
            "rcs0": np.concatenate(RCS0),
            "rcs1": np.concatenate(RCS1),
            "rcs2": np.concatenate(RCS2),
            "isBL": np.concatenate(y),
        }
    )

    if saveInCSV:
        if outputFile is None:
            outputFile = paths.file_labelleddataset()
        df.to_csv(outputFile, index=False)
        print("Dataset for ADABL is saved in", outputFile)

    return df


def generate_algokey(clf):
    """Generate a key to identify the classification algorithm used in ADABL
    
    It is used to correctly name the file in which trained model is stored. The
    naming convention used depends on the algorithm.
    
    
    Parameters
    ----------
    clf : `sklearn` classifier with `predict` method
        Trained classifier. Can classify any new data according to his training
    
    
    Returns
    -------
    algokey : str
        Algorithm identification key
    """
    algo = str(clf).split("(")[0]
    
    if algo == "RandomForestClassifier":
        algokey = "_".join(
            ["RandomForest", "M"+str(clf.n_estimators), "D"+str(clf.max_depth)]
        )
    elif algo == "KNeighborsClassifier":
        algokey = str(clf.n_neighbors)+"NearestNeighbors"
    elif algo == "DecisionTreeClassifier":
        algokey = "_".join(
            ["DecisionTree", "D"+str(clf.max_depth)]
        )
    elif algo == "AdaBoostClassifier":
        algokey = "_".join(
            ["AdaBoost", "M"+str(clf.n_estimators), "D"+str(clf.base_estimator.max_depth)]
        )
    elif algo == "LabelSpreading":
        algokey = "LabelSpreading_"+clf.kernel
    else:
        raise ValueError("Not supported algorithm:", algo)
    
    return algokey


def generate_predkey(predictors):
    """Generate a key to identify the set of predictors used in ADABL
    
    It is used to correctly name the file in which trained model is stored..
    
    
    Parameters
    ----------
    predictors : list of str
        Data used in input of the classification.
    
    
    Returns
    -------
    predkey : str
        Predictors identification key
    """
    predkey=""
    
    if "sec0" in predictors:
        predkey += "t"
    if "alti" in predictors:
        predkey += "z"
    
    predkey +="RCS"
    for p in predictors:
        if p[:3]=="rcs":
            predkey +=p[3]
    
    return predkey


def train_adabl(
    dataset,
    algo: str = "AdaBoost",
    predictors: list = ["sec0", "alti", "rcs0"],
    saveResults: bool = False,
    outputDir: str = None,
):
    """Train the classifier ADABL on the full dataset.
    
    The dataset provided must have the same format as generated by
    prepare_supervised_dataset. Trained model and scaler are stored in Pickle
    objects. No accuracy estimation is made here. See random_crossval or
    block_crossval to do so.
    
    Parameters
    ----------
    dataset : {str,`pandas.DataFrame`}
        Training dataset, either directly given (`pandas.DataFrame` object) or
        the path to CSV file containing it.
    
    algo : {"rf", "knn", "dt", "ab", "ls"}, default = "ab"
        Classification algorithm selected.
        
        - {"rf", "RandomForest", "RandomForestClassifier"}, random forest
          classifer with 50 trees of maximum depth 3
          
        - {"knn", "nearestneighbors", "KNeighborsClassifier"}, 6 nearest neighbors
          classifier
        
        - {"dt", "DecisionTree", "DecisionTreeClassifier"}, decision tree
          classifer with maximum depth 5
        
        - {"ab", "adab", "AdaBoost", "AdaBoostClassifier"}, AdaBoost classifier
          with 200 trees of maximum depth 5
        
        - {"ls", "LabelSpreading"}, label spreading semi-supervised classifier
          with nearest-neighbors kernel
    
    predictors : list of str
        Data used in input of the classification. Data names are dataframe
        colums. Default is ["sec0", "alti", "rcs0"]
    
    saveResults : bool, default=False
        If True, the classifier and the scaler are stored in Pickle objects
        after training.
    
    outputDir : str, default=None
        Path to the directory where classifier and scaler are saved if
        saveResults=True. File names are automatically set.
    
    
    Returns
    -------
    clf : `sklearn` classifier with `predict` method
        Trained classifier. Can classify any new data according to his training
    
    scaler : `sklearn` scaler with `transform` method
        Trained scaler. Ensure the normalisation is the same on the new data
        as on the training data
    """

    if isinstance(dataset, pd.DataFrame):
        df = dataset
    elif os.path.isfile(dataset):
        df = pd.read_csv(dataset)
    else:
        raise ValueError(
            "Argument 'dataset' must be either a path or a pandas.DataFrame"
        )

    X = df.loc[:, predictors].values
    y = df.loc[:, "isBL"].values

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Instantiate classifiers
    # -----------------------
    if algo in ["rf", "RandomForest", "RandomForestClassifier"]:
        from sklearn.ensemble import RandomForestClassifier

        clf = RandomForestClassifier(n_estimators=50, max_depth=3)
    elif algo in ["knn", "nearestneighbors", "KNeighborsClassifier"]:
        from sklearn.neighbors import KNeighborsClassifier

        clf = KNeighborsClassifier(n_neighbors=6)
    elif algo in ["dt", "DecisionTree", "DecisionTreeClassifier"]:
        from sklearn.tree import DecisionTreeClassifier

        clf = DecisionTreeClassifier(max_depth=5)
    elif algo in ["ab", "adab", "AdaBoost", "AdaBoostClassifier"]:
        from sklearn.ensemble import AdaBoostClassifier
        from sklearn.tree import DecisionTreeClassifier

        clf = AdaBoostClassifier(
            base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=200
        )
    elif algo in ["ls", "LabelSpreading"]:
        from sklearn.semi_supervised import LabelSpreading

        clf = LabelSpreading(kernel="knn")
    else:
        raise ValueError("Not supported algorithm:", algo)

    # Fit supervised model
    # ---------------------
    print("Fitting", clf)
    clf.fit(X, y)
    
    if saveResults:
        
        if outputDir is None:
            outputDir = paths.dir_trainedadabl()
        
        modelFile = os.path.join(outputDir, "_".join([
            "Classifier",
            generate_algokey(clf),
            generate_predkey(predictors)+".pkl",
        ]))
        fc = open(modelFile, "wb")
        pickle.dump(clf, fc)
        
        scalerFile = os.path.join(outputDir, "_".join([
            "Scaler",
            generate_predkey(predictors)+".pkl",
        ]))
        fc = open(scalerFile, "wb")
        pickle.dump(scaler, fc)
    
    return clf, scaler


def random_crossval(
    dataset,
    predictors: list = ["sec0", "alti", "rcs0"],
    cv_test_size: float = 0.2,
    n_random_splits: int = 10,
    plot_on: bool = False,
):
    """Estimate accuracy and computing time with random-split cross-validation.
    
    List of tested algorithms: RandomForestClassifier, KNeighborsClassifier,
    DecisionTreeClassifier, AdaBoostClassifier,LabelSpreading (total 5)
    Parameters of each algorithms must be modified inside the function.
    
    
    Parameters
    ----------
    dataset : {str,`pandas.DataFrame`}
        Training dataset, either directly given (`pandas.DataFrame` object) or
        the path to CSV file containing it.
    
    predictors : list of str
        Data used in input of the classification. Data names are dataframe
        colums. Default is ["sec0", "alti", "rcs0"]
    
    cv_test_size : float in [0,1], default=0.2
        Proportion of the dataset used for testing the algorithm by
        cross-validation. Must be between 0 and 1.
    
    n_random_splits : int, default=10
        Number of time the random split between test and train is repeated
    
    plot_on : bool, default=False
        If False, all graphics are disabled
    
    
    Returns
    -------
    accuracies : ndarray of shape (5,n_random_splits)
        Mean accuracy score (proportion of well classified individuals, the 
        closer to 1 the better) for each classifier and each random split 
        
    chronos : ndarray of shape (5,n_random_splits)
        Time to train the classifier and compute the accuracy score
    
    classifiers_keys : list of str
        Names of the tested classifiers
    """

    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import AdaBoostClassifier
    from sklearn.semi_supervised import LabelSpreading

    # Load dataset
    # ------------

    if isinstance(dataset, pd.DataFrame):
        df = dataset
    elif os.path.isfile(dataset):
        df = pd.read_csv(dataset)
    else:
        raise ValueError(
            "Argument 'dataset' must be either a path or a pandas.DataFrame"
        )

    X = df.loc[:, predictors].values
    y = df.loc[:, "isBL"].values

    # Normalisation
    # -------------

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Instantiate classifiers
    # -----------------------
    n_weaks = 200
    tree_depth = 5

    rfc = RandomForestClassifier(n_estimators=n_weaks, max_depth=tree_depth)
    knn = KNeighborsClassifier(n_neighbors=6)
    dtc = DecisionTreeClassifier(max_depth=tree_depth)
    abc = AdaBoostClassifier(
        base_estimator=DecisionTreeClassifier(max_depth=tree_depth),
        n_estimators=n_weaks,
    )
    lsc = LabelSpreading(kernel="knn")

    # Summarize performances
    # ----------------------

    print("TRAINING CLASSIFIERS...")
    classifiers = [rfc, knn, dtc, abc, lsc]
    classifiers_keys = [str(clf).split("(")[0] for clf in classifiers]
    chronos = np.zeros((len(classifiers), n_random_splits))
    accuracies = np.zeros((len(classifiers), n_random_splits))

    for icl in range(len(classifiers)):
        clf = classifiers[icl]
        print("Classifier", icl, "/", len(classifiers), classifiers_keys[icl])
        for ird in range(n_random_splits):
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=cv_test_size, random_state=ird
            )

            t0 = time.time()  #::::::
            clf.fit(X_train, y_train)
            accuracies[icl, ird] = clf.score(X_test, y_test)
            t1 = time.time()  #::::::
            chronos[icl, ird] = t1 - t0

    if plot_on:
        graphics.estimator_quality(accuracies, chronos, classifiers_keys)

    return accuracies, chronos, classifiers_keys

def block_crossval(
    dataset,
    predictors: list = ["sec0", "alti", "rcs0"],
    n_folds: int = 8,
    plot_on: bool = False,
):
    """Estimate accuracy and computing time with random-split cross-validation.
    
    List of tested algorithms: RandomForestClassifier, KNeighborsClassifier,
    DecisionTreeClassifier, AdaBoostClassifier,LabelSpreading (total 5)
    Parameters of each algorithms must be modified inside the function.
    
    
    Parameters
    ----------
    dataset : {str,`pandas.DataFrame`}
        Training dataset, either directly given (`pandas.DataFrame` object) or
        the path to CSV file containing it.
    
    predictors : list of str
        Data used in input of the classification. Data names are dataframe
        colums. Default is ["sec0", "alti", "rcs0"]
    
    n_folds : int, default=8
        Number of time the random split between test and train is repeated
    
    plot_on : bool, default=False
        If False, all graphics are disabled
    
    
    Returns
    -------
    accuracies : ndarray of shape (5,n_random_splits)
        Mean accuracy score (proportion of well classified individuals, the 
        closer to 1 the better) for each classifier and each random split 
        
    chronos : ndarray of shape (5,n_random_splits)
        Time to train the classifier and compute the accuracy score
    
    classifiers_keys : list of str
        Names of the tested classifiers
    """
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import AdaBoostClassifier
    from sklearn.semi_supervised import LabelSpreading
    from sklearn.model_selection import GroupKFold

    # Load dataset
    # ------------

    if isinstance(dataset, pd.DataFrame):
        df = dataset
    elif os.path.isfile(dataset):
        df = pd.read_csv(dataset)
    else:
        raise ValueError(
            "Argument 'dataset' must be either a path or a pandas.DataFrame"
        )

    X = df.loc[:, predictors].values
    y = df.loc[:, "isBL"].values

    # Normalisation
    # -------------

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Instantiate classifiers
    # -----------------------
    n_weaks = 200
    tree_depth = 5

    rfc = RandomForestClassifier(n_estimators=n_weaks, max_depth=tree_depth)
    knn = KNeighborsClassifier(n_neighbors=6)
    dtc = DecisionTreeClassifier(max_depth=tree_depth)
    abc = AdaBoostClassifier(
        base_estimator=DecisionTreeClassifier(max_depth=tree_depth),
        n_estimators=n_weaks,
    )
    lsc = LabelSpreading(kernel="knn")

    # Summarize performances
    # ----------------------

    classifiers = [rfc, knn, dtc, abc, lsc]
    classifiers_keys = [str(clf).split("(")[0] for clf in classifiers]
    chronos = np.zeros((len(classifiers), n_folds))
    accuracies = np.zeros((len(classifiers), n_folds))

    for icl in range(len(classifiers)):
        clf = classifiers[icl]
        print("\nClassifier", icl, "/", len(classifiers), classifiers_keys[icl])
        
        gkf = GroupKFold(n_splits=n_folds)
        group=np.zeros_like(y)
        for itx in df.groupby(np.floor(df.sec0/(24*3600/n_folds))).indices.items():
            grval,grdex = itx
            group[grdex]=int(grval)
        
        # setup toolbar
        toolbar_width = int(n_folds)
        sys.stdout.write(
            "   \_Group"+str(n_folds)+"Fold: [%s]" % ("." * toolbar_width)
        )
        sys.stdout.flush()
        sys.stdout.write("\b" * (toolbar_width + 1))  # return to start of line, after '['
        for ird, (idx_train, idx_test) in enumerate(gkf.split(X=X, y=y, groups=group)):
            sys.stdout.write("#")
            sys.stdout.flush()
            X_train = X[idx_train,:]
            X_test = X[idx_test,:]
            y_train = y[idx_train]
            y_test = y[idx_test]

            t0 = time.time()  #::::::
            clf.fit(X_train, y_train)
            accuracies[icl, ird] = clf.score(X_test, y_test)
            t1 = time.time()  #::::::
            chronos[icl, ird] = t1 - t0
    
    sys.stdout.write("]\n")
    
    if plot_on:
        graphics.estimator_quality(accuracies, chronos, classifiers_keys)
    
    return accuracies, chronos, classifiers_keys

def adabl_blh_estimation(
    dataFile: str,
    modelFile: str,
    scalerFile: str,
    outputFile: bool = None,
    storeInNetcdf: bool = False,
):
    """Perform BLH estimation with ADABL on all profiles of the day and 
    write it into a copy of the netcdf file
    
    
    Parameters
    ----------
    dataFile : str
        Path to the input file, as generated by raw2l1
    
    modelFile : str
        Path to the model file (pickle object)
    
    scalerFile : str
        Path to the scaler file (pickle object)
    
    outputFile : str
        Path to the output file. Default adds ".out" before ".nc"
    
    storeInNetcdf : bool
        If True, the field 'blh_ababl', containg BLH estimation, is stored in
        the outputFile
    
    
    Returns
    -------
    blh : ndarray of shape (Nt,)
        Time series of BLH as estimated by the ADABL algorithm.
    """

    t0 = time.time()  #::::::::::::::::::::::

    # 1. Extract the data
    # ---------------------
    loc, dateofday, lat, lon = utils.where_and_when(dataFile)
    t_values, z_values, dat = utils.extract_data(
        dataFile, to_extract=["rcs_1", "rcs_2", "pbl"]
    )
    rcs_1 = dat["rcs_1"]
    rcs_2 = dat["rcs_2"]
    blh_mnf = dat["pbl"]
    sec_intheday = np.mod(t_values, 24 * 3600)

    Nt, Nz = rcs_1.shape

    # Load pre-trained model
    # ------------------------
    fc = open(modelFile, "rb")
    model = pickle.load(fc)
    fc = open(scalerFile, "rb")
    scaler = pickle.load(fc)

    blh = []

    # setup toolbar
    toolbar_width = int(len(t_values) / 10) + 1
    sys.stdout.write(
        "ADABL estimation ("
        + loc
        + dateofday.strftime(", %Y/%m/%d")
        + "): [%s]" % ("." * toolbar_width)
    )
    sys.stdout.flush()
    sys.stdout.write("\b" * (toolbar_width + 1))  # return to start of line, after '['

    # Loop on all profile of the day
    for t in range(Nt):
        # toolbar
        if np.mod(t, 10) == 0:
            if any(np.isnan(blh[-11:-1])):
                sys.stdout.write("!")
            else:
                sys.stdout.write("*")
            sys.stdout.flush()

        # 2. Prepare the data
        # ---------------------
        rcs1loc = rcs_1[t, :]
        rcs2loc = rcs_2[t, :]
        rcs1loc[rcs1loc <= 0] = 1e-5
        rcs2loc[rcs2loc <= 0] = 1e-5

        X_new = np.array(
            [
                np.repeat(sec_intheday[t], Nz),
                z_values,
                np.log10(rcs1loc),
                np.log10(rcs2loc),
            ]
        ).T
        X_new = scaler.transform(X_new)

        # 3. Apply the machine learning algorithm
        # ---------------------
        y_new = model.predict(X_new)

        # 4. Derive and store the BLH
        # ---------------------
        blh.append(utils.blh_from_labels(y_new, z_values))

    # end toolbar
    t1 = time.time()  #::::::::::::::::::::::
    chrono = t1 - t0
    sys.stdout.write("] (" + str(np.round(chrono, 4)) + " s)\n")

    if outputFile is None:
        outputFile = dataFile[:-3] + ".out.nc"

    # 5. Store the new BLH estimation into a copy of the original netCDF
    if storeInNetcdf:
        utils.add_blh_to_netcdf(dataFile, outputFile, blh, origin="adabl")

    return np.array(blh)


def adabl_qualitymetrics(
    dataFile: str,
    modelFile: str,
    scalerFile: str,
    refFile: str = "indus",
    outputFile: str = "None",
    addResultsToNetcdf: bool = False,
):
    """Perform BLH estimation with ADABL on all profiles of the day and 
    write it into a copy of the netcdf file
    
    
    Parameters
    ----------
    dataFile : str
        Path to the input file, as generated by raw2l1
    
    modelFile : str
        Path to the model file (pickle object)
    
    scalerFile : str
        Path to the scaler file (pickle object)
    
    refFile : str
        Path to reference BLH estimation (handmade of manufacturer's). Default
        is the manufacturer.
    
    outputFile : str
        Path to the output file. Must be specified if addResultsToNetcdf=True
    
    addResultsToNetcdf : bool, default=False
        If True, adds the quality metrics to the existing result file specified 
        in outputFile
    
    
    
    Returns
    -------
    errl2_blh : float
        Root mean squared gap between BLH from KABL and the reference
        .. math:: \sqrt{1/N \sum_i^N (Z(i)-Zref(i))^2}
    
    errl1_blh : float
        Mean absolute gap between BLH from KABL and the reference
        .. math:: 1/N \sum_i^N \vert Z(i)-Zref(i) \vert
      
    errl0_blh : float
        Maximum absolute gap between BLH from KABL and the reference
        .. math:: \max_i \vert Z(i)-Zref(i) \vert
    
    ch_score : float
        Average Calinski-Harabasz score (the higher, the better) over
        the full day
        
    db_scores : float
        Average Davies-Bouldin score (the lower, the better) over
        the full day
    
    s_scores : float
        Average silhouette score (the higher, the better) over
        the full day
    
    chrono : float
        Computation time for the full day (seconds)
    
    n_invalid : int
        Number of BLH estimation at NaN or Inf
        """
    
    t0 = time.time()  #::::::::::::::::::::::

    # 1. Extract the data
    # ---------------------
    loc, dateofday, lat, lon = utils.where_and_when(dataFile)
    t_values, z_values, dat = utils.extract_data(
        dataFile, to_extract=["rcs_1", "rcs_2", "pbl"]
    )
    rcs_1 = dat["rcs_1"]
    rcs_2 = dat["rcs_2"]
    blh_mnf = dat["pbl"]
    sec_intheday = np.mod(t_values, 24 * 3600)

    Nt, Nz = rcs_1.shape

    # Load pre-trained model
    # ------------------------
    fc = open(modelFile, "rb")
    model = pickle.load(fc)
    fc = open(scalerFile, "rb")
    scaler = pickle.load(fc)

    blh = []

    # setup toolbar
    toolbar_width = int(len(t_values) / 10) + 1
    sys.stdout.write(
        "ADABL estimation ("
        + loc
        + dateofday.strftime(", %Y/%m/%d")
        + "): [%s]" % ("." * toolbar_width)
    )
    sys.stdout.flush()
    sys.stdout.write("\b" * (toolbar_width + 1))  # return to start of line, after '['

    # Loop on all profile of the day
    for t in range(Nt):
        # toolbar
        if np.mod(t, 10) == 0:
            if any(np.isnan(blh[-11:-1])):
                sys.stdout.write("!")
            else:
                sys.stdout.write("*")
            sys.stdout.flush()

        # 2. Prepare the data
        # ---------------------
        rcs1loc = rcs_1[t, :]
        rcs2loc = rcs_2[t, :]
        rcs1loc[rcs1loc <= 0] = 1e-5
        rcs2loc[rcs2loc <= 0] = 1e-5

        X_new = np.array(
            [
                np.repeat(sec_intheday[t], Nz),
                z_values,
                np.log10(rcs1loc),
                np.log10(rcs2loc),
            ]
        ).T
        X_new = scaler.transform(X_new)

        # 3. Apply the machine learning algorithm
        # ---------------------
        y_new = model.predict(X_new)

        # 4. Derive and store the BLH
        # ---------------------
        blh.append(utils.blh_from_labels(y_new, z_values))

    # end toolbar
    t1 = time.time()  #::::::::::::::::::::::
    chrono = t1 - t0
    sys.stdout.write("] (" + str(np.round(chrono, 4)) + " s)\n")

    if os.path.isfile(refFile):
        blh_ref = np.loadtxt(refFile)
    else:
        blh_ref = blh_mnf[:, 0]

    if addResultsToNetcdf:
        BLHS = [np.array(blh)]
        BLH_NAMES = ["BLH_ADABL"]

        msg = add_blhs_to_netcdf(outputFile, BLHS, BLH_NAMES)
        print(msg)

    errl2_blh = np.sqrt(np.nanmean((blh - blh_ref) ** 2))
    errl1_blh = np.nanmean(np.abs(blh - blh_ref))
    errl0_blh = np.nanmax(np.abs(blh - blh_ref))
    corr_blh = np.corrcoef(blh, blh_ref)[0, 1]
    n_invalid = np.sum(np.isnan(blh)) + np.sum(np.isinf(blh))

    return errl2_blh, errl1_blh, errl0_blh, corr_blh, chrono, n_invalid
