#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MODULE RELATIVE TO SUPERVISED LEARNING FUNCTIONS FOR THE KABL PROGRAM.

 +-----------------------------------------+
 |  Date of creation: 25 Nov. 2019         |
 +-----------------------------------------+
 |  Meteo-France                           |
 |  DSO/DOA/IED and CNRM/GMEI/LISA         |
 +-----------------------------------------+
 
Copyright Meteo-France, 2019, [CeCILL-C](https://cecill.info/licences.en.html) license (open source)

This module is a computer program that is part of the KABL (K-means for 
Atmospheric Boundary Layer) program. This program performs boundary layer
height estimation for concentration profiles using K-means algorithm.

This software is governed by the CeCILL-C license under French law and
abiding by the rules of distribution of free software.  You can  use,
modify and/ or redistribute the software under the terms of the CeCILL-C 
license as circulated by CEA, CNRS and INRIA at the following URL
"http://www.cecill.info".

As a counterpart to the access to the source code and  rights to copy,
modify and redistribute granted by the license, users are provided only
with a limited warranty  and the software's author,  the holder of the
economic rights,  and the successive licensors  have only  limited
liability.

In this respect, the user's attention is drawn to the risks associated
with loading,  using,  modifying and/or developing or reproducing the
software by the user in light of its specific status of free software,
that may mean  that it is complicated to manipulate,  and  that  also
therefore means  that it is reserved for developers  and  experienced
professionals having in-depth computer knowledge. Users are therefore
encouraged to load and test the software's suitability as regards their
requirements in conditions enabling the security of their systems and/or 
data to be ensured and,  more generally, to use and operate it in the
same conditions as regards security.

The fact that you are presently reading this means that you have had
knowledge of the CeCILL-C license and that you accept its terms.
"""

# Usual Python packages
import numpy as np
import pandas as pd
import datetime as dt
import sys
import os.path
import time
import pickle
import netCDF4 as nc

# Machine learning packages
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# Local packages
from kabl import core
from kabl import utils
from kabl import graphics


def add_blhs_to_netcdf(ncFile, blhs, blhs_names):
    """Add new fields to a netCDF file.
    
    [IN]
        - ncFile (str): path to the netcdf file containing the data
        - blhs (list of np.array): list of fields to add
        - blhs_names (list of str): list of fields' names
    
    [OUT] None
    """

    ncf = nc.Dataset(ncFile, "r+")

    # Fill in BLHs vectors
    for ib in range(len(blhs)):
        BLH = ncf.createVariable(blhs_names[ib], np.float64, ("time",))
        BLH[:] = blhs[ib][:]
        BLH.units = "Meters above ground level (m agl)"

    ncf.close()

    return "Fields " + str(blhs_names) + " successfully added to " + ncFile

def prepare_supervised_dataset(
    dataFiles: list,
    refFiles: list,
    saveInCSV: bool=False,
    outputFile: str="adabl_supervised_dataset.csv",
    plot_on: bool=False
):
    """Train the classifier ADABL
    
    [IN]
      - dataFile (str): path to the data input file, as generated by raw2l1
      - refFile (str): path to the reference file, if any.
    
    [OUT]
    """
    
    RCS0=[]
    RCS1=[]
    RCS2=[]
    SEC0=[]
    ALTI=[]
    y=[]
    for i in range(len(dataFiles)):
        dataFile = dataFiles[i]
        refFile = refFiles[i]
        print("Reading file ",dataFile,'with reference',refFile)
        t_values,z_values,rcs_0,rcs_1,rcs_2,blh_mnf=utils.extract_data(
            dataFile,
            max_height=4620,
            to_extract=['rcs_0','rcs_1','rcs_2','pbl']
        )
        
        blh_ref=pd.read_csv(refFile,delimiter=',',header=0)
        blh_ref=blh_ref['blh_ref'].values 
        
        if plot_on:
            graphics.blhs_over_data(t_values,z_values,rcs_0,blh_ref)
        
        # Input
        #-------
        sec_intheday=np.mod(t_values,24*3600)
        Nt,Nz=rcs_1.shape
        
        rcs0loc=rcs_0.ravel()
        rcs0loc[rcs0loc<=0]=1e-5
        RCS0.append(np.log10(rcs0loc))
        
        rcs1loc=rcs_1.ravel()
        rcs1loc[rcs1loc<=0]=1e-5
        RCS1.append(np.log10(rcs1loc))
        
        rcs2loc=rcs_2.ravel()
        rcs2loc[rcs2loc<=0]=2e-5
        RCS2.append(np.log10(rcs2loc))
        
        SEC0.append(np.repeat(sec_intheday,Nz))
        ALTI.append(np.tile(z_values,Nt))
        
        # Output
        #--------
        yday = []
        for t in range(Nt):
            yloc=np.zeros(Nz)
            yloc[z_values>blh_ref[t]]=1
            yday.append(yloc)
        
        y.append(np.array(yday,dtype=int).ravel())
        
    # Create dataframe
    #------------------
    df = pd.DataFrame(
            {
                'sec0':np.concatenate(SEC0),
                'alti':np.concatenate(ALTI),
                'rcs0':np.concatenate(RCS0),
                'rcs1':np.concatenate(RCS1),
                'rcs2':np.concatenate(RCS2),
                'isBL':np.concatenate(y)
            }
        )
    
    if saveInCSV:
        df.to_csv(outputFile,index=False)
        print("Dataset for ADABL is saved in",outputFile)
    
    return df


def train_adabl(
    dataset,
    algo: str='AdaBoost',
    predictors: list=['sec0','alti','rcs0'],
    outputFile: str="adabl_supervised_dataset.csv",
    plot_on: bool=False
):
    """Train the classifier ADABL
    
    [IN]
      - dataFile (str): path to the data input file, as generated by raw2l1
      - refFile (str): path to the reference file, if any.
    
    [OUT]
    """
    
    if isinstance(dataset,pd.DataFrame):
        df = dataset
    elif os.path.isfile(dataset):
        df = pd.read_csv(dataset)
    else:
        raise ValueError("Argument 'dataset' must be either a path or a pandas.DataFrame")
    
    X = df.loc[:,predictors].values
    y = df.loc[:,'isBL'].values
    
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    
    # Instantiate classifiers
    # -----------------------
    if algo in ['rf','RandomForest','RandomForestClassifier']:
        from sklearn.ensemble import RandomForestClassifier
        clf = RandomForestClassifier(n_estimators=50,max_depth=3)
    elif algo in ['knn','nearestneighbors','KNeighborsClassifier']:
        from sklearn.neighbors import KNeighborsClassifier
        clf = KNeighborsClassifier(n_neighbors=6)
    elif algo in ['dt','DecisionTree','DecisionTreeClassifier']:
        from sklearn.tree import DecisionTreeClassifier
        clf = DecisionTreeClassifier(max_depth=5)
    elif algo in ['ab','adab','AdaBoost','AdaBoostClassifier']:
        from sklearn.ensemble import AdaBoostClassifier
        from sklearn.tree import DecisionTreeClassifier
        clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=5),n_estimators=200)
    elif algo in ['ls','LabelSpreading']:
        from sklearn.semi_supervised import LabelSpreading
        clf = LabelSpreading(kernel='knn')
    else:
        raise ValueError("Not supported algorithm:",algo)
        
    
    # Fit supervised model
    # ---------------------
    print("Fitting",clf)
    clf.fit(X,y)
    
    return clf,scaler


def traintest_adabl(
    dataset,
    predictors: list=['sec0','alti','rcs0'],
    cv_test_size: float=0.2,
    n_random_splits: int=10,
    plot_on: bool=False
):
    '''Train and test several algorithms on the specified dataset.
    The outputs are thus the performance of each algorithms.
    
    List of tested algorithms: RandomForestClassifier,KNeighborsClassifier,
    DecisionTreeClassifier,AdaBoostClassifier,LabelSpreading (total 5)
    Parameters of each algorithms must be modified inside the function.
    
    [IN]
        - dataset (str): path to the dataset with identified labels
        - cv_test_size (float): proportion of the dataset used for testing the algorithm by cross-validation. Must be between 0 and 1.
        - n_random_splits (int): number of time the random split between test and train is repeated
        - plot_on (bool): if False, all graphics are disabled
    
    [OUT]
        - accuracies (np.array[5,n_random_splits]): mean accuracy score (proportion of well classified individuals, the closer to 1 the better) for each classifier and each random split 
        - chronos (np.array[5,n_random_splits]): time to train the classifier and compute the accuracy score
        - classifiers_keys (list[5] of str): names of the tested classifiers
    '''
    
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import AdaBoostClassifier
    from sklearn.semi_supervised import LabelSpreading
    
    # Load dataset
    # ------------
    
    if isinstance(dataset,pd.DataFrame):
        df = dataset
    elif os.path.isfile(dataset):
        df = pd.read_csv(dataset)
    else:
        raise ValueError("Argument 'dataset' must be either a path or a pandas.DataFrame")
    
    X = df.loc[:,predictors].values
    y = df.loc[:,'isBL'].values
    
    
    # Normalisation
    # -------------
    
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    
    
    # Instantiate classifiers
    # -----------------------
    n_weaks=200
    tree_depth=5
    
    rfc = RandomForestClassifier(n_estimators=n_weaks,max_depth=tree_depth)
    knn=KNeighborsClassifier(n_neighbors=6)
    dtc = DecisionTreeClassifier(max_depth=tree_depth)
    abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=tree_depth),n_estimators=n_weaks)
    lsc = LabelSpreading(kernel='knn')
    
    
    # Summarize performances
    # ----------------------
    
    print('TRAINING CLASSIFIERS...')
    classifiers =[rfc,knn,dtc,abc,lsc]
    classifiers_keys =[str(clf).split('(')[0] for clf in classifiers]
    chronos = np.zeros((len(classifiers),n_random_splits))
    accuracies = np.zeros((len(classifiers),n_random_splits))
    
    for icl in range(len(classifiers)):
        clf = classifiers[icl]
        print("Classifier",icl,"/",len(classifiers),classifiers_keys[icl])
        for ird in range(n_random_splits):
            X_train, X_test, y_train, y_test = train_test_split(
                X,
                y,
                test_size=cv_test_size,
                random_state=ird
            )
            
            t0=time.time()      #::::::
            clf.fit(X_train,y_train)
            accuracies[icl,ird]=clf.score(X_test,y_test)
            t1=time.time()      #::::::
            chronos[icl,ird]=t1-t0
    
    if plot_on:
        graphics.estimator_quality(accuracies,chronos,classifiers_keys)
    
    return accuracies,chronos,classifiers_keys

    
def adabl_blh_estimation(
    dataFile: str,
    modelFile: str,
    scalerFile: str,
    outputFile: bool=None,
    storeInNetcdf: bool=False
):
    """Perform BLH estimation with ADABL on all profiles of the day and 
    write it into a copy of the netcdf file
    
    [IN]
      - dataFile (str): path to the input file, as generated by raw2l1
      - modelFile (str): path to the model file (pickle object)
      - scalerFile (str): path to the scaler file (pickle object)
      - outputFile (str): path to the output file. Default adds ".out" before ".nc"
      - storeInNetcdf (bool): if True, the field 'blh_ababl', containg BLH estimation, is stored in the outputFile
    
    [OUT]
      - blh (np.array[Nt]): time series of BLH as estimated by the ADABL algorithm.
    """

    t0 = time.time()  #::::::::::::::::::::::

    # 1. Extract the data
    # ---------------------
    loc, dateofday, lat, lon = utils.where_and_when(dataFile)
    t_values, z_values, rcs_1, rcs_2, blh_mnf = utils.extract_data(
        dataFile, to_extract=["rcs_1", "rcs_2", "pbl"]
    )
    sec_intheday = np.mod(t_values, 24 * 3600)

    Nt, Nz = rcs_1.shape

    # Load pre-trained model
    # ------------------------
    fc = open(modelFile, "rb")
    model = pickle.load(fc)
    fc = open(scalerFile, "rb")
    scaler = pickle.load(fc)

    blh = []

    # setup toolbar
    toolbar_width = int(len(t_values) / 10) + 1
    sys.stdout.write(
        "ADABL estimation ("
        + loc
        + dateofday.strftime(", %Y/%m/%d")
        + "): [%s]" % ("." * toolbar_width)
    )
    sys.stdout.flush()
    sys.stdout.write("\b" * (toolbar_width + 1))  # return to start of line, after '['

    # Loop on all profile of the day
    for t in range(Nt):
        # toolbar
        if np.mod(t, 10) == 0:
            if any(np.isnan(blh[-11:-1])):
                sys.stdout.write("!")
            else:
                sys.stdout.write("*")
            sys.stdout.flush()

        # 2. Prepare the data
        # ---------------------
        rcs1loc = rcs_1[t, :]
        rcs2loc = rcs_2[t, :]
        rcs1loc[rcs1loc <= 0] = 1e-5
        rcs2loc[rcs2loc <= 0] = 1e-5

        X_new = np.array(
            [
                np.repeat(sec_intheday[t], Nz),
                z_values,
                np.log10(rcs1loc),
                np.log10(rcs2loc),
            ]
        ).T
        X_new = scaler.transform(X_new)

        # 3. Apply the machine learning algorithm
        # ---------------------
        y_new = model.predict(X_new)

        # 4. Derive and store the BLH
        # ---------------------
        blh.append(core.blh_from_labels(y_new, z_values))

    # end toolbar
    t1 = time.time()  #::::::::::::::::::::::
    chrono = t1 - t0
    sys.stdout.write("] (" + str(np.round(chrono, 4)) + " s)\n")

    if outputFile is None:
        outputFile = dataFile[:-3] + ".out.nc"

    # 5. Store the new BLH estimation into a copy of the original netCDF
    if storeInNetcdf:
        utils.add_blh_to_netcdf(dataFile, outputFile, blh, origin="adabl")

    return np.array(blh)


def adabl_qualitymetrics(
    dataFile: str,
    modelFile: str,
    scalerFile: str,
    refFile: str="indus",
    outputFile: str='None',
    addResultsToNetcdf: bool=False
):
    """Copy of blh_estimation including calculus and storage of scores
    
    [IN]
      - dataFile (str): path to the data input file, as generated by raw2l1
      - modelFile (str): path to the model file (pickle object)
      - scalerFile (str): path to the scaler file (pickle object)
      - outputFile (str): path to the output file. Default adds ".out" before ".nc"
      - refFile (str): path to the reference file, if any.
    
    [OUT]
      - errl2_blh (float): root mean squared gap between BLH from KABL and the reference
      - errl1_blh (float): mean absolute gap between BLH from KABL and the reference
      - errl0_blh (float): maximum absolute gap between BLH from KABL and the reference
      - ch_score (float): mean over all day Calinski-Harabasz score (the higher, the better)
      - db_scores (float): mean over all day Davies-Bouldin score (the lower, the better)
      - s_scores (float): mean over all day silhouette score (the higher, the better)
      - chrono (float): computation time for the full day (seconds)
      - n_invalid (int): number of BLH estimation at NaN or Inf
    """

    t0 = time.time()  #::::::::::::::::::::::

    # 1. Extract the data
    # ---------------------
    loc, dateofday, lat, lon = utils.where_and_when(dataFile)
    t_values, z_values, rcs_1, rcs_2, blh_mnf = utils.extract_data(
        dataFile, to_extract=["rcs_1", "rcs_2", "pbl"]
    )
    sec_intheday = np.mod(t_values, 24 * 3600)

    Nt, Nz = rcs_1.shape

    # Load pre-trained model
    # ------------------------
    fc = open(modelFile, "rb")
    model = pickle.load(fc)
    fc = open(scalerFile, "rb")
    scaler = pickle.load(fc)

    blh = []

    # setup toolbar
    toolbar_width = int(len(t_values) / 10) + 1
    sys.stdout.write(
        "ADABL estimation ("
        + loc
        + dateofday.strftime(", %Y/%m/%d")
        + "): [%s]" % ("." * toolbar_width)
    )
    sys.stdout.flush()
    sys.stdout.write("\b" * (toolbar_width + 1))  # return to start of line, after '['

    # Loop on all profile of the day
    for t in range(Nt):
        # toolbar
        if np.mod(t, 10) == 0:
            if any(np.isnan(blh[-11:-1])):
                sys.stdout.write("!")
            else:
                sys.stdout.write("*")
            sys.stdout.flush()

        # 2. Prepare the data
        # ---------------------
        rcs1loc = rcs_1[t, :]
        rcs2loc = rcs_2[t, :]
        rcs1loc[rcs1loc <= 0] = 1e-5
        rcs2loc[rcs2loc <= 0] = 1e-5

        X_new = np.array(
            [
                np.repeat(sec_intheday[t], Nz),
                z_values,
                np.log10(rcs1loc),
                np.log10(rcs2loc),
            ]
        ).T
        X_new = scaler.transform(X_new)

        # 3. Apply the machine learning algorithm
        # ---------------------
        y_new = model.predict(X_new)

        # 4. Derive and store the BLH
        # ---------------------
        blh.append(core.blh_from_labels(y_new, z_values))

    # end toolbar
    t1 = time.time()  #::::::::::::::::::::::
    chrono = t1 - t0
    sys.stdout.write("] (" + str(np.round(chrono, 4)) + " s)\n")

    if os.path.isfile(refFile):
        blh_ref = np.loadtxt(refFile)
    else:
        blh_ref = blh_mnf[:, 0]

    if addResultsToNetcdf:
        BLHS = [np.array(blh)]
        BLH_NAMES = ["BLH_ADABL"]

        msg = add_blhs_to_netcdf(outputFile, BLHS, BLH_NAMES)
        print(msg)

    errl2_blh = np.sqrt(np.nanmean((blh - blh_ref) ** 2))
    errl1_blh = np.nanmean(np.abs(blh - blh_ref))
    errl0_blh = np.nanmax(np.abs(blh - blh_ref))
    corr_blh = np.corrcoef(blh, blh_ref)[0, 1]
    n_invalid = np.sum(np.isnan(blh)) + np.sum(np.isinf(blh))

    return errl2_blh, errl1_blh, errl0_blh, corr_blh, chrono, n_invalid


########################
#      TEST BENCH      #
########################
# Launch with
# >> python graphics.py
#
# For interactive mode
# >> python -i graphics.py
#
if __name__ == "__main__":
    
    # Test of prepare_supervised_dataset
    # ------------------------
    print("\n --------------- Test of prepare_supervised_dataset")
    lidarDir="../data_samples/lidar/"
    dataFiles = [lidarDir+"DAILY_MPL_5025_20180802.nc",lidarDir+"DAILY_MPL_5029_20180224.nc"]
    refDir="../data_samples/handmade_BLH/"
    refFiles=[refDir+"blhref_Trappes_20180802.csv",refDir+"blhref_Brest_20180224.csv"]
    
    df=prepare_supervised_dataset(dataFiles,refFiles,saveInCSV=True)
    print("df=",df)
    
    # Test of train_adabl
    # ------------------------
    print("\n --------------- Test of train_adabl")
    
    model,scaler=train_adabl(df)
    print(
        "model=",model,'\n',
        "scaler=",scaler
    )
    
    # Test of traintest_adabl
    # ------------------------
    print("\n --------------- Test of traintest_adabl")
    
    accuracies,chronos,classifiers_keys=traintest_adabl(df,plot_on=True)
    print(
        "accuracies=",accuracies,'\n',
        "classifiers_keys=",classifiers_keys
    )
    
    # Test of adabl_blh_estimation
    # ------------------------
    print("\n --------------- Test of adabl_blh_estimation")
    dataFile = "../data_samples/lidar/DAILY_MPL_5025_20180802.nc"
    modelFile='../pre-trained-adabl/adabl_classifier_tzRCS12_M200_D5.pkl'
    scalerFile='../pre-trained-adabl/adabl_scaler_tzRCS12_M200_D5.pkl'
    
    blh_kabl=adabl_blh_estimation(dataFile,modelFile,scalerFile,storeInNetcdf=False)
    print("blh_kabl.shape",blh_kabl.shape)
    
    
    # Test of adabl_qualitymetrics
    # ------------------------
    print("\n --------------- Test of adabl_qualitymetrics")
    dataFile = "../data_samples/lidar/DAILY_MPL_5025_20180802.nc"
    modelFile='../pre-trained-adabl/adabl_classifier_tzRCS12_M200_D5.pkl'
    scalerFile='../pre-trained-adabl/adabl_scaler_tzRCS12_M200_D5.pkl'
    
    errl2_blh, errl1_blh, errl0_blh, corr_blh, chrono, n_invalid = adabl_qualitymetrics(dataFile,modelFile,scalerFile,)
    print(
        "Err L2:",errl2_blh,
        "Err L1:",errl1_blh,
        "Err L0:",errl0_blh,
        "Corr. :",corr_blh,
        "Chrono:",chrono,
        "#Inv:",n_invalid
    )
