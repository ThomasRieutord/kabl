#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
MODULE RELATIVE TO SUPERVISED LEARNING FUNCTIONS FOR THE KABL PROGRAM.

 +-----------------------------------------+
 |  Date of creation: 25 Nov. 2019         |
 +-----------------------------------------+
 |  Meteo-France                           |
 |  DSO/DOA/IED and CNRM/GMEI/LISA         |
 +-----------------------------------------+
 
Copyright Meteo-France, 2019, [CeCILL-C](https://cecill.info/licences.en.html) license (open source)

This module is a computer program that is part of the KABL (K-means for 
Atmospheric Boundary Layer) program. This program performs boundary layer
height estimation for concentration profiles using K-means algorithm.

This software is governed by the CeCILL-C license under French law and
abiding by the rules of distribution of free software.  You can  use,
modify and/ or redistribute the software under the terms of the CeCILL-C 
license as circulated by CEA, CNRS and INRIA at the following URL
"http://www.cecill.info".

As a counterpart to the access to the source code and  rights to copy,
modify and redistribute granted by the license, users are provided only
with a limited warranty  and the software's author,  the holder of the
economic rights,  and the successive licensors  have only  limited
liability.

In this respect, the user's attention is drawn to the risks associated
with loading,  using,  modifying and/or developing or reproducing the
software by the user in light of its specific status of free software,
that may mean  that it is complicated to manipulate,  and  that  also
therefore means  that it is reserved for developers  and  experienced
professionals having in-depth computer knowledge. Users are therefore
encouraged to load and test the software's suitability as regards their
requirements in conditions enabling the security of their systems and/or 
data to be ensured and,  more generally, to use and operate it in the
same conditions as regards security.

The fact that you are presently reading this means that you have had
knowledge of the CeCILL-C license and that you accept its terms.
'''

# Usual Python packages
import numpy as np
import datetime as dt
import sys
import os.path
import time
import pickle
import netCDF4 as nc

# Machine learning packages
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import AdaBoostClassifier

# Local packages
from . import core
from . import utils
from . import graphics

def add_field_to_netcdf(nc_file,blhs,blhs_names):
    '''Add new fields to a netCDF file.
    
    [IN]
        - nc_file (str): path to the netcdf file containing the data
        - blhs (list of np.array): list of fields to add
        - blhs_names (list of str): list of fields' names
    
    [OUT] None
    '''
    
    ncf=nc.Dataset(nc_file,'r+')
    
    # Fill in BLHs vectors
    for ib in range(len(blhs)):
        BLH = ncf.createVariable(blhs_names[ib],np.float64, ('time',))
        BLH[:] = blhs[ib][:]
        BLH.units = "Meters above ground level (m agl)"
    
    ncf.close()
    
    return "Fields "+str(blhs_names)+" successfully added to "+nc_file



def adabl_blh_estimation(inputFile,model_file,scaler_file,outputFile=None,storeInNetcdf=False):
    '''Perform BLH estimation with ADABL on all profiles of the day and 
    write it into a copy of the netcdf file
    
    [IN]
      - inputFile (str): path to the input file, as generated by raw2l1
      - model_file (str): path to the model file (pickle object)
      - scaler_file (str): path to the scaler file (pickle object)
      - outputFile (str): path to the output file. Default adds ".out" before ".nc"
      - storeInNetcdf (bool): if True, the field 'blh_ababl', containg BLH estimation, is stored in the outputFile
    
    [OUT]
      - blh (np.array[Nt]): time series of BLH as estimated by the ADABL algorithm.
    '''
    
    t0=time.time()      #::::::::::::::::::::::
    
    
    # 1. Extract the data
    #---------------------
    loc,dateofday,lat,lon=utils.where_and_when(inputFile)
    t_values,z_values,rcs_1,rcs_2,blh_mnf=utils.extract_data(inputFile,to_extract=['rcs_1','rcs_2','pbl'])
    sec_intheday=np.mod(t_values,24*3600)
    
    Nt,Nz=rcs_1.shape
    
    # Load pre-trained model
    #------------------------
    fc = open(model_file, 'rb') 
    model = pickle.load(fc) 
    fc = open(scaler_file, 'rb') 
    scaler = pickle.load(fc)
    
    blh = []
    
    # setup toolbar
    toolbar_width = int(len(t_values)/10)+1
    sys.stdout.write("ADABL estimation ("+loc+dateofday.strftime(', %Y/%m/%d')+"): [%s]" % ("." * toolbar_width))
    sys.stdout.flush()
    sys.stdout.write("\b" * (toolbar_width+1)) # return to start of line, after '['
    
    # Loop on all profile of the day
    for t in range(Nt):
        # toolbar
        if np.mod(t,10)==0:
            if any(np.isnan(blh[-11:-1])):
                sys.stdout.write("!")
            else:
                sys.stdout.write("*")
            sys.stdout.flush()
        
        # 2. Prepare the data
        #---------------------
        rcs1loc=rcs_1[t,:]
        rcs2loc=rcs_2[t,:]
        rcs1loc[rcs1loc<=0]=1e-5
        rcs2loc[rcs2loc<=0]=1e-5
        
        X_new=np.array([np.repeat(sec_intheday[t],Nz),z_values,np.log10(rcs1loc),np.log10(rcs2loc)]).T
        X_new = scaler.transform(X_new)
        
        # 3. Apply the machine learning algorithm
        #---------------------
        y_new = model.predict(X_new)
        
        # 4. Derive and store the BLH
        #---------------------
        blh.append(core.blh_from_labels(y_new,z_values))
            
    
    # end toolbar
    t1=time.time()      #::::::::::::::::::::::
    chrono=t1-t0
    sys.stdout.write("] ("+str(np.round(chrono,4))+" s)\n")
    
    if outputFile is None:
        outputFile=inputFile[:-3]+".out.nc"
    
    # 5. Store the new BLH estimation into a copy of the original netCDF
    if storeInNetcdf:
        utils.add_blh_to_netcdf(inputFile,outputFile,blh,origin='adabl')
    
    return np.array(blh)

def adabl_qualitymetrics(inputFile,model_file,scaler_file,outputFile,reference='None',addResultsToNetcdf=False):
    '''Copy of blh_estimation including calculus and storage of scores
    
    [IN]
      - inputFile (str): path to the input file, as generated by raw2l1
      - model_file (str): path to the model file (pickle object)
      - scaler_file (str): path to the scaler file (pickle object)
      - outputFile (str): path to the output file. Default adds ".out" before ".nc"
      - reference (str): path to the reference file, if any.
    
    [OUT]
      - errl2_blh (float): root mean squared gap between BLH from KABL and the reference
      - errl1_blh (float): mean absolute gap between BLH from KABL and the reference
      - errl0_blh (float): maximum absolute gap between BLH from KABL and the reference
      - ch_score (float): mean over all day Calinski-Harabasz score (the higher, the better)
      - db_scores (float): mean over all day Davies-Bouldin score (the lower, the better)
      - s_scores (float): mean over all day silhouette score (the higher, the better)
      - chrono (float): computation time for the full day (seconds)
      - n_invalid (int): number of BLH estimation at NaN or Inf
    '''
    
    t0=time.time()      #::::::::::::::::::::::
    
    
    # 1. Extract the data
    #---------------------
    loc,dateofday,lat,lon=utils.where_and_when(inputFile)
    t_values,z_values,rcs_1,rcs_2,blh_mnf=utils.extract_data(inputFile,to_extract=['rcs_1','rcs_2','pbl'])
    sec_intheday=np.mod(t_values,24*3600)
    
    Nt,Nz=rcs_1.shape
    
    # Load pre-trained model
    #------------------------
    fc = open(model_file, 'rb') 
    model = pickle.load(fc) 
    fc = open(scaler_file, 'rb') 
    scaler = pickle.load(fc)
    
    blh = []
    
    # setup toolbar
    toolbar_width = int(len(t_values)/10)+1
    sys.stdout.write("ADABL estimation ("+loc+dateofday.strftime(', %Y/%m/%d')+"): [%s]" % ("." * toolbar_width))
    sys.stdout.flush()
    sys.stdout.write("\b" * (toolbar_width+1)) # return to start of line, after '['
    
    # Loop on all profile of the day
    for t in range(Nt):
        # toolbar
        if np.mod(t,10)==0:
            if any(np.isnan(blh[-11:-1])):
                sys.stdout.write("!")
            else:
                sys.stdout.write("*")
            sys.stdout.flush()
        
        # 2. Prepare the data
        #---------------------
        rcs1loc=rcs_1[t,:]
        rcs2loc=rcs_2[t,:]
        rcs1loc[rcs1loc<=0]=1e-5
        rcs2loc[rcs2loc<=0]=1e-5
        
        X_new=np.array([np.repeat(sec_intheday[t],Nz),z_values,np.log10(rcs1loc),np.log10(rcs2loc)]).T
        X_new = scaler.transform(X_new)
        
        # 3. Apply the machine learning algorithm
        #---------------------
        y_new = model.predict(X_new)
        
        # 4. Derive and store the BLH
        #---------------------
        blh.append(core.blh_from_labels(y_new,z_values))
            
    
    # end toolbar
    t1=time.time()      #::::::::::::::::::::::
    chrono=t1-t0
    sys.stdout.write("] ("+str(np.round(chrono,4))+" s)\n")
    
    if os.path.isfile(reference):
        blh_ref=np.loadtxt(reference)
    else:
        blh_ref=blh_mnf[:,0]
    
    if addResultsToNetcdf:
        BLHS = [np.array(blh)]
        BLH_NAMES = ['BLH_ADABL']
        
        msg=add_field_to_netcdf(outputFile,BLHS,BLH_NAMES)
        print(msg)
    
    errl2_blh=np.sqrt(np.nanmean((blh-blh_ref)**2))
    errl1_blh=np.nanmean(np.abs(blh-blh_ref))
    errl0_blh=np.nanmax(np.abs(blh-blh_ref))
    corr_blh=np.corrcoef(blh,blh_ref)[0,1]
    n_invalid=np.sum(np.isnan(blh))+np.sum(np.isinf(blh))
    
    return errl2_blh,errl1_blh,errl0_blh,corr_blh,chrono,n_invalid
